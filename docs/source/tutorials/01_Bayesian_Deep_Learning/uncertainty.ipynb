{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantifying uncertainty to enhance decision making is the main benefit of Bayesian Deep Learning (BDL). This is particularly useful when the model encounters data that it hasn't seen during training, which we demonstrated in the [Bayesian Deep Learning Tutorial](bayesian_deep_learning.ipynb) where ensembles are trained on data within (-2pi, 2pi) and tested on (-8pi, 8pi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](de_regression_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that the predictions for data within the training set (between the dotted lines) are closely clustered around the mean indicating a high level of certainty. Predictions on data outside the training set produce a distribution of results with much higher levels of variance, indicating high uncertainty. \n",
    "\n",
    "In this tutorial, we will quantify uncertainty on a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be examining the MNIST dataset, which consists of 60,000 28x28 pixel grayscale images. We will train on the original images, and test on a set of rotated images to demonstrate the model's ability to handle data outside the training set. Our expectation should be that the more rotated/augmented the test image is, the more **uncertain** our model will become."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](mnist.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.nns.simplenet.simplenet\n",
    "from experiments.nns.simplenet.simplenet import SimpleNet\n",
    "from experiments.nns.cnn.cnn import CNN\n",
    "from experiments.nns.lenet.lenet import LeNet\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current notebook's path\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Define the path to directory containing MNIST\n",
    "mnist_directory = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(notebook_path)), \"..\",\"..\",\"..\",\"..\",\"..\",\"..\",\"..\", \"/usr/data1/vision/data/\"))\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the MNIST training dataset\n",
    "train_dataset = datasets.MNIST(root=mnist_directory, train=True, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reduces the size of the training set to 1/10th the original amount, to decrease training time.\n",
    "#  This cell only needs to be run once. After first run, indices are saved in MNIST directory.\n",
    "\n",
    "\n",
    "# # Select 1/10 samples of each class for both train and test datasets\n",
    "# train_subset_indices = []\n",
    "\n",
    "# for c in range(10):\n",
    "#     class_indices = [i for i in range(len(train_dataset)) if train_dataset[i][1] == c]\n",
    "#     train_subset_indices.extend(class_indices[:len(class_indices)//10])\n",
    "\n",
    "# # # Save the subset indices inside mnist_directory\n",
    "# train_idx_path = os.path.join(mnist_directory, \"train_indices.pth\")\n",
    "# torch.save(train_subset_indices, train_idx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_path = os.path.join(mnist_directory, \"train_indices.pth\")\n",
    "\n",
    "# Load the subset indices from mnist_directory\n",
    "loaded_train_indices = torch.load(train_idx_path)\n",
    "\n",
    "# Create the subset using the loaded indices\n",
    "loaded_train_subset = torch.utils.data.Subset(train_dataset, loaded_train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Rotated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from bdl import CustomMNISTDataset\n",
    "\n",
    "# Select the numbers we would like to create rotated versions of\n",
    "selected_numbers = [1]\n",
    "# Include rotation in the transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomRotation(degrees=(-180, 180), fill=(0,)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "# ])\n",
    "\n",
    "# rotation_angles = [0, 15, 30, 45, 60, 75, 80, 95]\n",
    "rotation_angles = []\n",
    "for i in range(12):\n",
    "    rotation_angles.append(i*5.5)\n",
    "def get_rotated_mnist_dataset(degrees):\n",
    "    rotate_transform = transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=(degrees, degrees), fill=(0,)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "    rotated_dataset = CustomMNISTDataset(root=mnist_directory, numbers=selected_numbers, train=False, transform=rotate_transform)\n",
    "    rotated_loader = DataLoader(rotated_dataset, batch_size=512, shuffle=False)\n",
    "    return rotated_dataset, rotated_loader\n",
    "rotated_datasets = []\n",
    "rotated_loaders = []\n",
    "\n",
    "for angle in rotation_angles:\n",
    "    rotated_dataset, rotated_loader = get_rotated_mnist_dataset(angle)\n",
    "    rotated_datasets.append(rotated_dataset)\n",
    "    rotated_loaders.append(rotated_loader)\n",
    "\n",
    "\n",
    "test_dataset = CustomMNISTDataset(root=mnist_directory, numbers=selected_numbers, train=False, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(loaded_train_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDL Methods\n",
    "We will be applying three different BDL methods to compare each methods ability to quantify uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Ensembles\n",
    "Deep ensembles run the same model under different intialiazations or random seeds to generate a distribution of weights.\n",
    "It treats the ensemble of models as an approximation to the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-SWAG\n",
    "Multi-SWAG creates a distribution of parameter states by \n",
    "\n",
    "1. Calculating a running average for each parameter (μ)\n",
    "2. Tracking the deviation from the mean for each parameter for the last K states with K=20 by default (σ)\n",
    "\n",
    "and approximates the posterior by applying Bayesian Model Averaging to samples taken from the Gaussian ~ N(μ,σ²). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import push.bayes.ensemble\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "ensemble = push.bayes.ensemble.train_deep_ensemble(\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(),\n",
    "        epochs,\n",
    "        LeNet,\n",
    "        num_devices=2,\n",
    "        num_ensembles=100,\n",
    "        cache_size=25\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "push_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
