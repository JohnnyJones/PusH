{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Bayesian Deep Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we introduce Bayesian Deep Learning (BDL) and demonstrate how to use Push to perform BDL by running a deep ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: The Posterior Predictive Distribution\n",
    "\n",
    "The goal of BDL methods is to compute the *posterior predictive distribution*\n",
    "$$\n",
    "p(y|x, \\mathcal{D}) = \\int p(y|x, \\theta) p(\\theta | \\mathcal{D}) \\, d\\theta\n",
    "$$\n",
    "where $y$ is an output, $x$ is an input, $\\theta$ are parameters, and $\\mathcal{D} = (x_i, y_i)_{i}$ is a dataset. In general, this integral is intractable and must be approximated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Approximation\n",
    "\n",
    "We can approximate the posterior predictive distribution with a Monte Carlo approximation [1]: \n",
    "$$\n",
    "p(y|x, \\mathcal{D}) \\approx \\frac{1}{J} \\sum_{j=1}^{J} p(y|x, \\theta_j), \\quad \\theta_j \\sim p(\\theta | D)\n",
    "$$\n",
    "where each $\\theta_j$ corresponds to a unique parameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Ensembles\n",
    "\n",
    " *Deep ensembles* train $J$ different initializations of the same neural network (NN). If we average $p(y | x, \\theta_j)$ over $J$ parameter settings $\\theta_j$, we are essentially performing a Monte Carlo estimate that approximates the posterior predictive distribution. Thus, deep ensembles form a simple BDL method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Ensembles in Push\n",
    "\n",
    "We will now introduce deep ensembles in Push. Push interoperates with PyTorch, and so all datasets and models can use PyTorch components. Push also has concurrent execution semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For illustrative purposes, we create a random dataset with $N$ points of dimension $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bdl.RandDataset at 0x7f1a383d8cd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bdl import RandDataset\n",
    "\n",
    "D = 1\n",
    "batch_size = 128\n",
    "N = 1\n",
    "dataset = RandDataset(batch_size, N, D)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "class RandDataset(Dataset):\n",
    "    def __init__(self, batch_size, N, D):\n",
    "        self.xs = torch.randn(batch_size*N, D)\n",
    "        self.ys = torch.randn(batch_size*N, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "We create a simple neural network (NN). This NN contains two fully-connected layers of dimension $D$ and uses a ReLU activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bdl import MiniNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "class MiniNN(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(MiniNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(D, D)\n",
    "        self.fc2 = nn.Linear(D, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Deep Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss 0 tensor(1.6703)\n",
      "Average loss 0 tensor(1.6702)\n",
      "Average loss 0 tensor(1.6702)\n",
      "Average loss 0 tensor(1.6702)\n",
      "Average loss 0 tensor(1.6701)\n",
      "Average loss 0 tensor(1.6701)\n",
      "Average loss 0 tensor(1.6700)\n",
      "Average loss 0 tensor(1.6700)\n",
      "Average loss 0 tensor(1.6699)\n",
      "Average loss 0 tensor(1.6699)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[tensor([[-0.8776]]),\n",
       "  tensor([-0.0283]),\n",
       "  tensor([[-0.9058]]),\n",
       "  tensor([-0.4190])],\n",
       " [tensor([[-0.1410]]), tensor([0.1787]), tensor([[0.9782]]), tensor([0.3471])],\n",
       " [tensor([[-0.7838]]), tensor([0.2805]), tensor([[0.0788]]), tensor([0.4403])],\n",
       " [tensor([[0.0092]]),\n",
       "  tensor([-0.4970]),\n",
       "  tensor([[-0.9888]]),\n",
       "  tensor([0.5064])]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import push.bayes.ensemble\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "num_ensembles = 4\n",
    "push.bayes.ensemble.train_deep_ensemble(\n",
    "    dataloader,\n",
    "    torch.nn.MSELoss(),\n",
    "    epochs,\n",
    "    MiniNN, D,\n",
    "    num_devices=1,\n",
    "    num_ensembles=num_ensembles\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] A.G. Wilson, P. Izmailov. Bayesian Deep Learning and a Probabilistic Perspective of Generalization. Advances in Neural Information Processing Systems, 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "push_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
