{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push BDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Neural networks are typically trained to find a single parameter setting that produces a point estimate for a given input. Larger models, like those found in Deep Neural Networks can represent the same function with many valid parameter settings, with each being independently trained. If we consider many different models trained on the same data, we can treat the range of predictions/parameter settings as a distribution and represent uncertainty through defining a metric of variation. This method of training multiple models is called a deep ensemble, and is the motivation for this notebook.\n",
    "\n",
    "Probabilistic Machine Learning: Advanced Topics, Kevin Murphy, Chapter 17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Posterior Predictive Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y|x, \\mathcal{D}) = \\int p(y|x, \\theta) p(\\theta | \\mathcal{D}) \\, d\\theta\n",
    "$$\n",
    "\n",
    "The posterior predictive distribution is a concept in Bayesian statistics that combines information from both the observed data and the uncertainty in the model parameters. It provides a way to make new predictions for new or future data points, taking into account what we've learned from the data we've already observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y|x, \\mathcal{D})$: This is the conditional probability of *y* given *x* and D. It represents the probability of some outcome *y* given both the input *x* and the observed data D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y|x,\\theta)$: This is the likelihood function. It represents the probabillity of observing the outcome *y* given the input *x* and a specific value of the parameter $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\theta|\\mathcal{D})$: This is the posterior distribution of the parameter $\\theta$ given the data *D*. It represents the updated probability distribution of $\\theta$ after taking into account the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* $p(\\theta|D) ∝ p(\\theta)p(D|\\theta)$\n",
    "\n",
    "This expression represents **Bayes theorem**\n",
    "\n",
    "$p(\\theta|D)$: This is the postierior probability, which represents the probability of the parameter $\\theta$ given the data *D*.\n",
    "\n",
    "$∝$: Means \"proportional to\", which in this context indicates the expression on the left is proportional to the right with a constant proportionality that ensures the total probability sums up to 1\n",
    "\n",
    "$p(\\theta)$: This is the **prior** probability representing the initial belief or probability distribution of the parameter $\\theta$ before observing any data\n",
    "\n",
    "$p(D|\\theta)$: This is the **likelihood**, which represents the probability of observing the data *D* given a specific value of the parameter $\\theta$. It describes how well the model with parameter $\\theta$ explains the observed data\n",
    "\n",
    "Putting this all together, Bayes theorem states the updated probability of $p(\\theta|D)$ is proportional to the product of the prior probability of the parameter $p(\\theta)$ and the likelihood of observing the data given that parameter $p(D|\\theta)$. Resulting in the following mathmatical expression.\n",
    "\n",
    "$p(\\theta|D) ∝ p(\\theta) * p(D|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y|x, \\mathcal{D}) = \\int p(y|x, \\theta) p(\\theta | \\mathcal{D}) \\, d\\theta\n",
    "$$\n",
    "Thus, this expression is stating that the conditional probability of *y* given *x* and *D* can be calculated by integrating the product of the likelihood $p(y|x, \\theta)$ and the posterior distribution $p(\\theta|D)$ over all possible values of $\\theta$. This is what we term the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a prior can be done explicitly, or implicitly. In the deep ensemble case, we can treat the architecture of the model itself as a prior. The number of parameters and hidden layers can be varied to create more distinct models, while different starting parameter values and random seeds introduce randomization between identical model runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Deep Ensemble in Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import push.bayes.ensemble\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "n = 4\n",
    "epochs = 100\n",
    "num_ensembles = 2\n",
    "two_particle_params = push.bayes.ensemble.train_deep_ensemble(\n",
    "    dataloader,\n",
    "    torch.nn.MSELoss(),\n",
    "    epochs,\n",
    "    BiggerNN, n, input_dim, output_dim, hidden_dim,\n",
    "    num_devices=1,\n",
    "    num_ensembles=num_ensembles,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "push_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
