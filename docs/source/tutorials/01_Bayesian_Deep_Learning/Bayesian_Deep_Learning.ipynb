{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Neural networks are typically trained to find a single parameter setting that produces a point estimate for a given input. Larger models, like those found in Deep Neural Networks can represent the same function with many valid parameter settings, with each being independently trained. If we consider many different models trained on the same data, we can treat the range of predictions/parameter settings as a distribution and represent uncertainty through defining a metric of variation. This method of training multiple models is called a deep ensemble, and is the motivation for this notebook.\n",
    "\n",
    "Probabilistic Machine Learning: Advanced Topics, Kevin Murphy, Chapter 17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Posterior Predictive Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y|x, \\mathcal{D}) = \\int p(y|x, \\theta) p(\\theta | \\mathcal{D}) \\, d\\theta\n",
    "$$\n",
    "\n",
    "The posterior predictive distribution is a concept in Bayesian statistics that combines information from both the observed data and the uncertainty in the model parameters. It provides a way to make new predictions for new or future data points, taking into account what we've learned from the data we've already observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y|x, \\mathcal{D})$: This is the conditional probability of *y* given *x* and D. It represents the probability of some outcome *y* given both the input *x* and the observed data D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y|x,\\theta)$: This is the likelihood function. It represents the probabillity of observing the outcome *y* given the input *x* and a specific value of the parameter $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\theta|\\mathcal{D})$: This is the posterior distribution of the parameter $\\theta$ given the data *D*. It represents the updated probability distribution of $\\theta$ after taking into account the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* $p(\\theta|D) ∝ p(\\theta)p(D|\\theta)$\n",
    "\n",
    "This expression represents **Bayes theorem**\n",
    "\n",
    "$p(\\theta|D)$: This is the postierior probability, which represents the probability of the parameter $\\theta$ given the data *D*.\n",
    "\n",
    "$∝$: Means \"proportional to\", which in this context indicates the expression on the left is proportional to the right with a constant proportionality that ensures the total probability sums up to 1\n",
    "\n",
    "$p(\\theta)$: This is the **prior** probability representing the initial belief or probability distribution of the parameter $\\theta$ before observing any data\n",
    "\n",
    "$p(D|\\theta)$: This is the **likelihood**, which represents the probability of observing the data *D* given a specific value of the parameter $\\theta$. It describes how well the model with parameter $\\theta$ explains the observed data\n",
    "\n",
    "Putting this all together, Bayes theorem states the updated probability of $p(\\theta|D)$ is proportional to the product of the prior probability of the parameter $p(\\theta)$ and the likelihood of observing the data given that parameter $p(D|\\theta)$. Resulting in the following mathmatical expression.\n",
    "\n",
    "$p(\\theta|D) ∝ p(\\theta) * p(D|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y|x, \\mathcal{D}) = \\int p(y|x, \\theta) p(\\theta | \\mathcal{D}) \\, d\\theta\n",
    "$$\n",
    "Thus, this expression is stating that the conditional probability of *y* given *x* and *D* can be calculated by integrating the product of the likelihood $p(y|x, \\theta)$ and the posterior distribution $p(\\theta|D)$ over all possible values of $\\theta$. This is what we term the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a prior can be done explicitly, or implicitly. In the deep ensemble case, we can treat the architecture of the model itself as a prior. The number of parameters and hidden layers can be varied to create more distinct models, while different starting parameter values and random seeds introduce randomization between identical model runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Deep Ensemble in Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from utils.LB_utils import * \n",
    "import utils.LB_utils_special as LB_utils_special\n",
    "from utils.load_not_MNIST import notMNIST\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import laplace\n",
    "\n",
    "s = 1\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.cuda.manual_seed(s)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imagenet\n",
    "#imagenet_train_root = os.path.abspath('your imagenet path')\n",
    "#imagenet_val_root = os.path.abspath('your imagenet path')\n",
    "\n",
    "# Get the current working directory (directory of your notebook)\n",
    "notebook_directory = os.path.dirname(os.path.abspath(\"01_Bayesian_Deep_Learning_Tutorial.ipynb\"))\n",
    "\n",
    "# Navigate to the parent folder (assuming \"usr\" and \"home\" are at the same level)\n",
    "parent_directory = os.path.abspath(os.path.join(notebook_directory, \"..\",\"..\",\"..\",\"..\",\"..\",\"..\",\"..\"))\n",
    "# Construct the path to the ImageNet directory\n",
    "imagenet_directory = os.path.abspath(os.path.join(parent_directory, \"/usr/data1/imagenet\"))\n",
    "\n",
    "# print(imagenet_directory)\n",
    "\n",
    "imagenet_train_root = os.path.abspath(imagenet_directory + '/train')\n",
    "imagenet_val_root = os.path.abspath(imagenet_directory + '/val')\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform_imagenet_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "transform_imagenet_val =  transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "imagenet_train = datasets.ImageFolder(imagenet_train_root, transform=transform_imagenet_train)\n",
    "indices_small = np.random.choice(np.arange(0, len(imagenet_train)), size=(20000,), replace=False)\n",
    "imagenet_train_small = torch.utils.data.Subset(imagenet_train, torch.tensor(indices_small))\n",
    "imagenet_val = datasets.ImageFolder(imagenet_val_root, transform=transform_imagenet_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        imagenet_train_small,\n",
    "        batch_size=16,\n",
    "        shuffle=True)\n",
    "\n",
    "train_loader_16 = torch.utils.data.DataLoader(\n",
    "        imagenet_train_small,\n",
    "        batch_size=16,\n",
    "        shuffle=True)\n",
    "\n",
    "train_loader_64 = torch.utils.data.DataLoader(\n",
    "        imagenet_train_small,\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "\n",
    "train_loader_128 = torch.utils.data.DataLoader(\n",
    "        imagenet_train_small,\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        imagenet_val,\n",
    "        batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.25, 2.0086104634371584]\n",
      "2.0086104634371584\n",
      "3.25\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import tueplots\n",
    "from tueplots import bundles\n",
    "plt.rcParams.update(tueplots.bundles.icml2022())\n",
    "\n",
    "print(plt.rcParams['figure.figsize'])\n",
    "figwidth = plt.rcParams['figure.figsize'][0]\n",
    "figheight = plt.rcParams['figure.figsize'][1]\n",
    "matplotlib.rcParams['font.family'] = \"serif\"\n",
    "matplotlib.rcParams['font.serif'] = 'Times new Roman'\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "print(figheight)\n",
    "print(figwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:44<00:00, 104.80s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReceiveFuncAckPDMSG' object has no attribute 'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#load densenet\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# densenet = densenet121(pretrained=False).cuda()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# densenet.eval()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m two_particle_params \u001b[39m=\u001b[39m push\u001b[39m.\u001b[39;49mbayes\u001b[39m.\u001b[39;49mensemble\u001b[39m.\u001b[39;49mtrain_deep_ensemble(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         train_loader_128,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         densenet121, \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         num_devices\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         num_ensembles\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsrv-hss-157-54.at.sfsu.edu/home/jtsegaye/push/docs/source/tutorials/01_Bayesian_Deep_Learning/Bayesian_Deep_Learning.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/push/push/bayes/ensemble.py:132\u001b[0m, in \u001b[0;36mtrain_deep_ensemble\u001b[0;34m(dataloader, loss_fn, epochs, nn, num_devices, cache_size, view_size, num_ensembles, mk_optim, ensemble_entry, ensemble_state, *args)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Train a deep ensemble PusH distribution and return a list of particle parameters.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m    List[torch.Tensor]: Returns a list of all particle's parameters.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m ensemble \u001b[39m=\u001b[39m Ensemble(nn, \u001b[39m*\u001b[39margs, num_devices\u001b[39m=\u001b[39mnum_devices, cache_size\u001b[39m=\u001b[39mcache_size, view_size\u001b[39m=\u001b[39mview_size)\n\u001b[0;32m--> 132\u001b[0m ensemble\u001b[39m.\u001b[39;49mbayes_infer(dataloader, epochs, loss_fn\u001b[39m=\u001b[39;49mloss_fn, num_ensembles\u001b[39m=\u001b[39;49mnum_ensembles, mk_optim\u001b[39m=\u001b[39;49mmk_optim,\n\u001b[1;32m    133\u001b[0m                      ensemble_entry\u001b[39m=\u001b[39;49mensemble_entry, ensemble_state\u001b[39m=\u001b[39;49mensemble_state)\n\u001b[1;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m ensemble\u001b[39m.\u001b[39mp_parameters()\n",
      "File \u001b[0;32m~/push/push/bayes/ensemble.py:99\u001b[0m, in \u001b[0;36mEnsemble.bayes_infer\u001b[0;34m(self, dataloader, epochs, loss_fn, num_ensembles, mk_optim, ensemble_entry, ensemble_state, f_save)\u001b[0m\n\u001b[1;32m     94\u001b[0m     pids \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush_dist\u001b[39m.\u001b[39mp_create(mk_optim, device\u001b[39m=\u001b[39m(n \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_devices), receive\u001b[39m=\u001b[39m{\n\u001b[1;32m     95\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mENSEMBLE_STEP\u001b[39m\u001b[39m\"\u001b[39m: _ensemble_step,\n\u001b[1;32m     96\u001b[0m     }, state\u001b[39m=\u001b[39m{})]\n\u001b[1;32m     98\u001b[0m \u001b[39m# 2. Perform independent training\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpush_dist\u001b[39m.\u001b[39;49mp_wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpush_dist\u001b[39m.\u001b[39;49mp_launch(\u001b[39m0\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mENSEMBLE_MAIN\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataloader, loss_fn, epochs)])\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m f_save:\n\u001b[1;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush_dist\u001b[39m.\u001b[39msave()\n",
      "File \u001b[0;32m~/push/push/push.py:279\u001b[0m, in \u001b[0;36mPusH.p_wait\u001b[0;34m(self, futures)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_wait\u001b[39m(\u001b[39mself\u001b[39m, futures: \u001b[39mlist\u001b[39m[PFuture]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39many\u001b[39m]:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pwait([future\u001b[39m.\u001b[39;49m_fid \u001b[39mfor\u001b[39;49;00m future \u001b[39min\u001b[39;49;00m futures])\n",
      "File \u001b[0;32m~/push/push/push.py:147\u001b[0m, in \u001b[0;36mPusH._pwait\u001b[0;34m(self, fids)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mraise\u001b[39;00m msg\n\u001b[1;32m    146\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, ReceiveFuncAckPDMSG):\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results[msg\u001b[39m.\u001b[39mpid_fid[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    148\u001b[0m loop()\n\u001b[1;32m    149\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReceiveFuncAckPDMSG' object has no attribute 'result'"
     ]
    }
   ],
   "source": [
    "import models\n",
    "from models.densenet import densenet121\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import push.bayes.ensemble\n",
    "\n",
    "#load densenet\n",
    "\n",
    "# densenet = densenet121(pretrained=False).cuda()\n",
    "# densenet.eval()\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "two_particle_params = push.bayes.ensemble.train_deep_ensemble(\n",
    "        train_loader_128,\n",
    "        torch.nn.CrossEntropyLoss(),\n",
    "        epochs,\n",
    "        densenet121, False,\n",
    "        num_devices=2,\n",
    "        num_ensembles=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "push_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
