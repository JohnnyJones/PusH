{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Aleatoric (data) vs Epistemic (model) Uncertainty\n",
    "Uncertainty can arise from noisy data, model architecture, incomplete data, etc. and generally can be classified as either model or data uncertainty. From them modeler's point of view, we are most concerned with model uncertainty since we are able to improve our models through training of a DNN.\n",
    "\n",
    "[1] A Survey of Uncertainty in Deep Neural Networks. Figure 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y^*|x^*, D) = \\int \\underbrace{p(y^*|x^*, \\theta)}_{\\text{Data}} \\underbrace{p(\\theta|D)}_{\\text{Model}} \\, d\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Aleatoric Uncertainty\n",
    "Aleoric (data) uncertainty can be traced to errors in data measurement, and is caused by a loss of information when translating real world observations into measurable ones. In a classification task with the MNIST dataset, data uncertainty could be caused by lower resolution versions of the images. In a regression task predicting various qualities of wine, any inaccuracies caused by the measuring equipment, weather, or human error cannot be learned by the neural network. Data uncertainty is considered irreducible due to the imperfect manner in which we collect data and conduct experiments. This introduces an unavoidable element of randomness that causes variability in experiment outcomes. This means we can only estimate data uncertainty, since reducing it is not a possibility.\n",
    "\n",
    "[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1](fuzzymnist.png)\n",
    "![Figure 2](winequality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] Generative models and Bayesian inversion using Laplace approximation Figure 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Epistemic Uncertainty\n",
    "Epistemic uncertainty is caused my the model, which is why its referred to as model uncertainty. Model uncertainty is typically encapsulated by errors in architecture design, training procedures, or insufficient training data . As the modeler, we have full control over both the architecture and training procudure, thus model uncertainty is reducible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Approaches to Modeling Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Single Deterministic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Multi-Swag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 Stein Variational Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ensemble Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Simple Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Test Time Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Uncertainty Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 In Domain Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Domain Shift Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Out of Domain Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References \n",
    "\n",
    "[1] Gawlikowski, J., Njieutcheu Tassi, C. R., Ali, M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R., Shahzad, M., Yang, W., Bamler, R., & Zhu, X. X. (2022). A Survey of Uncertainty in Deep Neural Networks. arXiv preprint arXiv:2107.03342.\n",
    "\n",
    "[2] Marschall, M., Wübbeler, G., Schmähling, F., & Elster, C. (2023). Generative models and Bayesian inversion using Laplace approximation. Computational Statistics, Advance online publication. https://doi.org/10.1007/s00180-023-01345-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "push_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
